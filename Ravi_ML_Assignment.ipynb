{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = r'G:\\RAVISIR\\dataset1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 3,491,778\n",
      "Trainable params: 3,469,890\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 240 images belonging to 2 classes.\n",
      "Found 96 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7/7 [==============================] - 6s 801ms/step - loss: 1.1047 - acc: 0.8113 - val_loss: 0.0953 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.95833, saving model to C:\\Users\\COE1\\top_model_weights.h5\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 3s 412ms/step - loss: 0.2983 - acc: 0.8879 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.95833 to 0.96875, saving model to C:\\Users\\COE1\\top_model_weights.h5\n",
      "\n",
      "Starting to Fine Tune Model\n",
      "\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 564ms/step - loss: 0.2457 - acc: 0.9107 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96875, saving model to C:\\Users\\COE1\\model_weights.h5\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 3s 425ms/step - loss: 0.3575 - acc: 0.8809 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 3s 445ms/step - loss: 0.0748 - acc: 0.9730 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 3s 487ms/step - loss: 0.2572 - acc: 0.9420 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 3s 435ms/step - loss: 0.2199 - acc: 0.9084 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 3s 456ms/step - loss: 0.2976 - acc: 0.9369 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 3s 466ms/step - loss: 0.1763 - acc: 0.9549 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 3s 468ms/step - loss: 0.2851 - acc: 0.9207 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 4s 590ms/step - loss: 0.2087 - acc: 0.9375 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 3s 427ms/step - loss: 0.2210 - acc: 0.9544 - val_loss: 0.0680 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 27 16:04:09 2018\n",
    "\n",
    "@author: Ravi\n",
    "\"\"\"\n",
    " \n",
    "import sys \n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.applications import *\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as k\n",
    "import keras\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "# fix seed for reproducible results (only works on CPU, not GPU)\n",
    "seed = 9\n",
    "np.random.seed(seed=seed)\n",
    "tf.set_random_seed(seed=seed)\n",
    "\n",
    "# hyper parameters for model\n",
    "nb_classes = 2  # number of classes\n",
    "based_model_last_block_layer_number = 126  # value is based on based model selected.\n",
    "#img_width, img_height = 299, 299  # change based on the shape/structure of your images for inception etc\n",
    "img_width, img_height = 224, 224  # change based on the shape/structure of your images for squeezenet\n",
    "\n",
    "batch_size = 32  # try 4, 8, 16, 32, 64, 128, 256 dependent on CPU/GPU memory capacity (powers of 2 values).\n",
    "nb_epoch = 10  # number of iteration the algorithm gets trained.\n",
    "learn_rate = 1e-4  # sgd learning rate\n",
    "momentum = .9  # sgd momentum to avoid local minimum\n",
    "transformation_ratio = .05  # how aggressive will be the data augmentation/transformation\n",
    "nb_train_samples = 240  # Total number of train samples. NOT including augmented images\n",
    "nb_validation_samples = 96  # Total number of train samples. NOT including augmented images.\n",
    "\n",
    "\n",
    "def train(train_data_dir, validation_data_dir, model_path):\n",
    "    # Pre-Trained CNN Model using imagenet dataset for pre-trained weights\n",
    "    # base_model = Xception(input_shape=(img_width, img_height, 3), weights='imagenet', include_top=False)\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first': \n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "    \n",
    "    base_model = MobileNet(input_shape=input_shape, weights='imagenet', include_top=False)\n",
    "\n",
    "    # # Top Model Block\n",
    "    # x = base_model.output\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    # predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "    \n",
    "    # add a global spatial average pooling layer\n",
    "    # x = base_model.output\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    # # let's add a fully-connected layer\n",
    "    # x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    predictions = Dense(nb_classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    # add your top layer block to your base model\n",
    "    model = Model(base_model.input, predictions)\n",
    "    print(model.summary())\n",
    "\n",
    "    # # let's visualize layer names and layer indices to see how many layers/blocks to re-train\n",
    "    # # uncomment when choosing based_model_last_block_layer\n",
    "    # for i, layer in enumerate(model.layers):\n",
    "    #     print(i, layer.name)\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all layers of the based model that is already pre-trained.\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Read Data and Augment it: Make sure to select augmentations that are appropriate to your images.\n",
    "    # To save augmentations un-comment save lines and add to your flow parameters.\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                       rotation_range=transformation_ratio,\n",
    "                                       shear_range=transformation_ratio,\n",
    "                                       zoom_range=transformation_ratio,\n",
    "                                       cval=transformation_ratio,\n",
    "                                       horizontal_flip=True,\n",
    "                                       vertical_flip=True)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # os.makedirs(os.path.join(os.path.abspath(train_data_dir), 'preview'), exist_ok=True)\n",
    "    train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                        target_size=[img_width, img_height],\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')\n",
    "    # save_to_dir=os.path.join(os.path.abspath(train_data_dir), '../preview')\n",
    "    # save_prefix='aug',\n",
    "    # save_format='jpeg')\n",
    "    # use the above 3 commented lines if you want to save and look at how the data augmentations look like\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                                  target_size=[img_width, img_height],\n",
    "                                                                  batch_size=batch_size,\n",
    "                                                                  class_mode='categorical')\n",
    "\n",
    "    model.compile(optimizer='nadam',\n",
    "                  loss='categorical_crossentropy',  # categorical_crossentropy if multi-class classifier\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # save weights of best training epoch: monitor either val_loss or val_acc\n",
    "\n",
    "    top_weights_path = os.path.join(os.path.abspath(model_path), 'top_model_weights.h5')\n",
    "    callbacks_list = [\n",
    "        ModelCheckpoint(top_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "        EarlyStopping(monitor='val_acc', patience=5, verbose=0),\n",
    "        keras.callbacks.TensorBoard(log_dir='tensorboard/inception-v3-train-top-layer', histogram_freq=0, write_graph=False, write_images=False)\n",
    "    ]\n",
    "\n",
    "    # Train Simple CNN\n",
    "    model.fit_generator(train_generator,\n",
    "                        steps_per_epoch=nb_train_samples // batch_size,\n",
    "                        epochs=nb_epoch / 5,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=nb_validation_samples // batch_size,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "    # verbose\n",
    "    print(\"\\nStarting to Fine Tune Model\\n\")\n",
    "\n",
    "    # add the best weights from the train top model\n",
    "    # at this point we have the pre-train weights of the base model and the trained weight of the new/added top model\n",
    "    # we re-load model weights to ensure the best epoch is selected and not the last one.\n",
    "    model.load_weights(top_weights_path)\n",
    "\n",
    "    # based_model_last_block_layer_number points to the layer in your model you want to train.\n",
    "    # For example if you want to train the last block of a 19 layer VGG16 model this should be 15\n",
    "    # If you want to train the last Two blocks of an Inception model it should be 172\n",
    "    # layers before this number will used the pre-trained weights, layers above and including this number\n",
    "    # will be re-trained based on the new data.\n",
    "    for layer in model.layers[:based_model_last_block_layer_number]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[based_model_last_block_layer_number:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # compile the model with a SGD/momentum optimizer\n",
    "    # and a very slow learning rate.\n",
    "    model.compile(optimizer='nadam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "                  \n",
    "    \n",
    "    # save weights of best training epoch: monitor either val_loss or val_acc\n",
    "    final_weights_path = os.path.join(os.path.abspath(model_path), 'model_weights.h5')\n",
    "    callbacks_list = [\n",
    "        ModelCheckpoint(final_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
    "        #EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "        keras.callbacks.TensorBoard(log_dir='tensorboard/inception-v3-fine-tune', histogram_freq=0, write_graph=False, write_images=False)\n",
    "    ]\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "    # fine-tune the model\n",
    "    model.fit_generator(train_generator,\n",
    "                        steps_per_epoch=nb_train_samples // batch_size,\n",
    "                        epochs=nb_epoch,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=nb_validation_samples // batch_size,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "    # save model\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(os.path.abspath(model_path), 'model.json'), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "\n",
    "train(r'G:\\RAVISIR\\dataset1\\train', r'G:\\RAVISIR\\dataset1\\validation', '.')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     if not len(sys.argv) == 3:\n",
    "#         print('Arguments must match:\\npython code/fine_tune.py <data_dir/> <model_dir/>')\n",
    "#         print('Example: python code/fine_tune.py data/dogs_cats/ model/dog_cats/')\n",
    "#         sys.exit(2)\n",
    "#     else:\n",
    "#         data_dir = os.path.abspath(sys.argv[1])\n",
    "#         train_dir = os.path.join(os.path.abspath(data_dir), 'train')  # Inside, each class should have it's own folder\n",
    "#         validation_dir = os.path.join(os.path.abspath(data_dir), 'validation')  # each class should have it's own folder\n",
    "#         model_dir = os.path.abspath(sys.argv[2])\n",
    "\n",
    "#         os.makedirs(os.path.join(os.path.abspath(data_dir), 'preview'), exist_ok=True)\n",
    "#         os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "#     train(train_dir, validation_dir, model_dir)  # train model\n",
    "\n",
    "#     # release memory\n",
    "#     k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
